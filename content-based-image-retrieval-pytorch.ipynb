{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:27.103132Z","iopub.status.busy":"2023-10-06T06:04:27.102819Z","iopub.status.idle":"2023-10-06T06:04:34.023086Z","shell.execute_reply":"2023-10-06T06:04:34.022062Z","shell.execute_reply.started":"2023-10-06T06:04:27.103103Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting barbar\n","  Downloading barbar-0.2.1-py3-none-any.whl (3.9 kB)\n","Collecting torchsummary\n","  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n","Installing collected packages: barbar, torchsummary\n","Successfully installed barbar-0.2.1 torchsummary-1.5.1\n","\u001b[33mWARNING: You are using pip version 20.1.1; however, version 23.2.1 is available.\n","You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["# Additional Dependencies\n","!pip install barbar torchsummary"]},{"cell_type":"markdown","metadata":{},"source":["# Content Based Image Retrieval (CBIR)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-06T06:04:34.025638Z","iopub.status.busy":"2023-10-06T06:04:34.025277Z","iopub.status.idle":"2023-10-06T06:04:36.224912Z","shell.execute_reply":"2023-10-06T06:04:36.224053Z","shell.execute_reply.started":"2023-10-06T06:04:34.025599Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import time\n","import copy\n","import pickle\n","from barbar import Bar\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import scipy\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","import cv2\n","%matplotlib inline\n","\n","import torch\n","import torchvision\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","from torchvision import transforms\n","from torchsummary import summary\n","\n","from tqdm import tqdm\n","from pathlib import Path\n","import gc\n","RANDOMSTATE = 0\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2023-10-06T06:04:36.226729Z","iopub.status.busy":"2023-10-06T06:04:36.226250Z","iopub.status.idle":"2023-10-06T06:04:36.279433Z","shell.execute_reply":"2023-10-06T06:04:36.278234Z","shell.execute_reply.started":"2023-10-06T06:04:36.226694Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["# Find if any accelerator is presented, if yes switch device to use CUDA or else use CPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:36.281457Z","iopub.status.busy":"2023-10-06T06:04:36.280905Z","iopub.status.idle":"2023-10-06T06:04:39.698562Z","shell.execute_reply":"2023-10-06T06:04:39.697708Z","shell.execute_reply.started":"2023-10-06T06:04:36.281418Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/kaggle/input/cbir-dataset/dataset/1269.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/kaggle/input/cbir-dataset/dataset/3863.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/kaggle/input/cbir-dataset/dataset/623.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/kaggle/input/cbir-dataset/dataset/2193.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/kaggle/input/cbir-dataset/dataset/3750.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         image\n","0  /kaggle/input/cbir-dataset/dataset/1269.jpg\n","1  /kaggle/input/cbir-dataset/dataset/3863.jpg\n","2   /kaggle/input/cbir-dataset/dataset/623.jpg\n","3  /kaggle/input/cbir-dataset/dataset/2193.jpg\n","4  /kaggle/input/cbir-dataset/dataset/3750.jpg"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# preparing intermediate DataFrame\n","datasetPath = Path('/kaggle/input/cbir-dataset/dataset/')\n","df = pd.DataFrame()\n","\n","df['image'] = [f for f in os.listdir(datasetPath) if os.path.isfile(os.path.join(datasetPath, f))]\n","df['image'] = '/kaggle/input/cbir-dataset/dataset/' + df['image'].astype(str)\n","\n","df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:39.701910Z","iopub.status.busy":"2023-10-06T06:04:39.701652Z","iopub.status.idle":"2023-10-06T06:04:39.706809Z","shell.execute_reply":"2023-10-06T06:04:39.705628Z","shell.execute_reply.started":"2023-10-06T06:04:39.701885Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(4738, 1)\n"]}],"source":["print(df.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preparation"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:39.710807Z","iopub.status.busy":"2023-10-06T06:04:39.710108Z","iopub.status.idle":"2023-10-06T06:04:39.718835Z","shell.execute_reply":"2023-10-06T06:04:39.717919Z","shell.execute_reply.started":"2023-10-06T06:04:39.710772Z"},"trusted":true},"outputs":[],"source":["class CBIRDataset(Dataset):\n","    def __init__(self, dataFrame):\n","        self.dataFrame = dataFrame\n","        \n","        self.transformations = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","    \n","    def __getitem__(self, key):\n","        if isinstance(key, slice):\n","            raise NotImplementedError('slicing is not supported')\n","        \n","        row = self.dataFrame.iloc[key]\n","        image = self.transformations(Image.open(row['image']))\n","        return image\n","    \n","    def __len__(self):\n","        return len(self.dataFrame.index)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:39.720813Z","iopub.status.busy":"2023-10-06T06:04:39.720145Z","iopub.status.idle":"2023-10-06T06:04:39.732175Z","shell.execute_reply":"2023-10-06T06:04:39.731228Z","shell.execute_reply.started":"2023-10-06T06:04:39.720636Z"},"trusted":true},"outputs":[],"source":["# Intermediate Function to process data from the data retrival class\n","def prepare_data(DF):\n","    trainDF, validateDF = train_test_split(DF, test_size=0.15, random_state=RANDOMSTATE)\n","    train_set = CBIRDataset(trainDF)\n","    validate_set = CBIRDataset(validateDF)\n","    \n","    return train_set, validate_set"]},{"cell_type":"markdown","metadata":{},"source":["# AutoEncoder Model"]},{"cell_type":"markdown","metadata":{},"source":["## High Level Structure of an AutoEncoder"]},{"cell_type":"markdown","metadata":{},"source":["![](https://hackernoon.com/hn-images/1*op0VO_QK4vMtCnXtmigDhA.png)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:39.734099Z","iopub.status.busy":"2023-10-06T06:04:39.733557Z","iopub.status.idle":"2023-10-06T06:04:39.744781Z","shell.execute_reply":"2023-10-06T06:04:39.743810Z","shell.execute_reply.started":"2023-10-06T06:04:39.734065Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class GeM(nn.Module):\n","    def __init__(self, p=3, eps=1e-6, requires_grad=False):\n","        super(GeM, self).__init__()\n","        self.p = nn.Parameter(torch.ones(1)*p, requires_grad=requires_grad)\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        return self.gem(x, p=self.p, eps=self.eps)\n","\n","    def gem(self, x, p=3, eps=1e-6):\n","#         return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n","        # Clamp the input tensor and raise it to the power 'p'\n","        x_pow = x.clamp(min=eps).pow(p)\n","\n","        # Perform average pooling on a 2x2 block\n","        x_avg_pool = F.avg_pool2d(x_pow, (3, 3), stride = (1,1), padding = (1, 1))\n","\n","        # Apply the final power operation\n","        gem_result = x_avg_pool.pow(1.0 / p)\n","\n","        return gem_result\n","\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:39.746541Z","iopub.status.busy":"2023-10-06T06:04:39.745971Z","iopub.status.idle":"2023-10-06T06:04:39.788716Z","shell.execute_reply":"2023-10-06T06:04:39.787966Z","shell.execute_reply.started":"2023-10-06T06:04:39.746478Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input shape: torch.Size([1, 256, 16, 16])\n","Output shape: torch.Size([1, 256, 16, 16])\n"]}],"source":["import torch\n","\n","# Create a sample input tensor\n","sample_input = torch.randn(1, 256, 16, 16)  # Batch size of 1, 256 channels, 16x16 spatial dimensions\n","\n","# Create an instance of the GeM layer\n","gem_layer = GeM(p=3, eps=1e-6)\n","\n","# Pass the input through the GeM layer\n","output = gem_layer(sample_input)\n","\n","# Print the input and output shapes\n","print(\"Input shape:\", sample_input.shape)\n","print(\"Output shape:\", output.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:39.790843Z","iopub.status.busy":"2023-10-06T06:04:39.790274Z","iopub.status.idle":"2023-10-06T06:04:39.801345Z","shell.execute_reply":"2023-10-06T06:04:39.800568Z","shell.execute_reply.started":"2023-10-06T06:04:39.790805Z"},"trusted":true},"outputs":[],"source":["class ConvAutoencoder(nn.Module):\n","    def __init__(self):\n","        super(ConvAutoencoder, self).__init__()\n","        self.encoder = nn.Sequential(# in- ( N,3,512,512)\n","            \n","            nn.Conv2d(in_channels=3, \n","                      out_channels=16, \n","                      kernel_size=(3,3), \n","                      stride=3, \n","                      padding=1),  # (32,16,171,171)\n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2),  # (N,16,85,85)\n","            \n","            nn.Conv2d(in_channels=16, \n","                      out_channels=8, \n","                      kernel_size=(3,3), \n","                      stride=2, \n","                      padding=1),  # (N,8,43,43)\n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=1)  # (N,8,42,42)\n","        )\n","        self.decoder = nn.Sequential(\n","            \n","            nn.ConvTranspose2d(in_channels = 8, \n","                               out_channels=16, \n","                               kernel_size=(3,3), \n","                               stride=2),  # (N,16,85,85)\n","            nn.ReLU(True),\n"," \n","            nn.ConvTranspose2d(in_channels=16, \n","                               out_channels=8, \n","                               kernel_size=(5,5), \n","                               stride=3, \n","                               padding=1),  # (N,8,255,255)\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(in_channels=8, \n","                               out_channels=3, \n","                               kernel_size=(6,6), \n","                               stride=2, \n","                               padding=1),  # (N,3,512,512)\n","            nn.Tanh()\n","        )\n","#         self.gem = GeMLayer()  # Add GeM layer as the final layer of the encoder\n","\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","#         x = self.gem(x)\n","        print(\"Encoder  : \", x.shape)\n","        x = self.decoder(x)\n","        print(\"Decoder : \", x.shape)\n","        return x"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:39.803722Z","iopub.status.busy":"2023-10-06T06:04:39.803096Z","iopub.status.idle":"2023-10-06T06:04:39.822291Z","shell.execute_reply":"2023-10-06T06:04:39.821465Z","shell.execute_reply.started":"2023-10-06T06:04:39.803651Z"},"trusted":true},"outputs":[],"source":["class ConvAutoencoder_v2(nn.Module):\n","    def __init__(self):\n","        super(ConvAutoencoder_v2, self).__init__()\n","        self.gem = GeM()\n","\n","        self.encoder = nn.Sequential(# in- (N,3,512,512)\n","            \n","            nn.Conv2d(in_channels=3, \n","                      out_channels=64, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=64, \n","                      out_channels=64, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1),\n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2), \n","            \n","            nn.Conv2d(in_channels=64, \n","                      out_channels=128, \n","                      kernel_size=(3,3), \n","                      stride=2, \n","                      padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=128, \n","                      out_channels=128, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=0), \n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2), \n","            \n","            nn.Conv2d(in_channels=128, \n","                      out_channels=256, \n","                      kernel_size=(3,3), \n","                      stride=2, \n","                      padding=1), \n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=256, \n","                      out_channels=256, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1), \n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=256, \n","                      out_channels=256, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1), \n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2) \n","#             GeM()\n","        )\n","        self.decoder = nn.Sequential(\n","            \n","            nn.ConvTranspose2d(in_channels = 256, \n","                               out_channels=256, \n","                               kernel_size=(3,3), \n","                               stride=1,\n","                              padding=1), \n"," \n","            nn.ConvTranspose2d(in_channels=256, \n","                               out_channels=256, \n","                               kernel_size=(3,3), \n","                               stride=1, \n","                               padding=1),  \n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(in_channels=256, \n","                               out_channels=128, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=0),  \n","            \n","            nn.ConvTranspose2d(in_channels=128, \n","                               out_channels=64, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=1),  \n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(in_channels=64, \n","                               out_channels=32, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=1), \n","            \n","            nn.ConvTranspose2d(in_channels=32, \n","                               out_channels=32, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=1),  \n","            nn.ReLU(True),\n","            \n","            nn.ConvTranspose2d(in_channels=32, \n","                               out_channels=3, \n","                               kernel_size=(4,4), \n","                               stride=2, \n","                               padding=2),  \n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x_enc = self.encoder(x)\n","#         x_enc = self.gem(x_enc)\n","#         print(\"Encoder output : \", x.shape)\n","        x_dec = self.decoder(x_enc)\n","#         print(\"Decoder output : \", x.shape)\n","        return x_enc, x_dec"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:39.824171Z","iopub.status.busy":"2023-10-06T06:04:39.823623Z","iopub.status.idle":"2023-10-06T06:04:43.696366Z","shell.execute_reply":"2023-10-06T06:04:43.695437Z","shell.execute_reply.started":"2023-10-06T06:04:39.824062Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 512, 512]           1,792\n","              ReLU-2         [-1, 64, 512, 512]               0\n","            Conv2d-3         [-1, 64, 512, 512]          36,928\n","              ReLU-4         [-1, 64, 512, 512]               0\n","         MaxPool2d-5         [-1, 64, 256, 256]               0\n","            Conv2d-6        [-1, 128, 128, 128]          73,856\n","              ReLU-7        [-1, 128, 128, 128]               0\n","            Conv2d-8        [-1, 128, 126, 126]         147,584\n","              ReLU-9        [-1, 128, 126, 126]               0\n","        MaxPool2d-10          [-1, 128, 63, 63]               0\n","           Conv2d-11          [-1, 256, 32, 32]         295,168\n","             ReLU-12          [-1, 256, 32, 32]               0\n","           Conv2d-13          [-1, 256, 32, 32]         590,080\n","             ReLU-14          [-1, 256, 32, 32]               0\n","           Conv2d-15          [-1, 256, 32, 32]         590,080\n","             ReLU-16          [-1, 256, 32, 32]               0\n","        MaxPool2d-17          [-1, 256, 16, 16]               0\n","  ConvTranspose2d-18          [-1, 256, 16, 16]         590,080\n","  ConvTranspose2d-19          [-1, 256, 16, 16]         590,080\n","             ReLU-20          [-1, 256, 16, 16]               0\n","  ConvTranspose2d-21          [-1, 128, 33, 33]         295,040\n","  ConvTranspose2d-22           [-1, 64, 65, 65]          73,792\n","             ReLU-23           [-1, 64, 65, 65]               0\n","  ConvTranspose2d-24         [-1, 32, 129, 129]          18,464\n","  ConvTranspose2d-25         [-1, 32, 257, 257]           9,248\n","             ReLU-26         [-1, 32, 257, 257]               0\n","  ConvTranspose2d-27          [-1, 3, 512, 512]           1,539\n","             Tanh-28          [-1, 3, 512, 512]               0\n","================================================================\n","Total params: 3,313,731\n","Trainable params: 3,313,731\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 3.00\n","Forward/backward pass size (MB): 678.39\n","Params size (MB): 12.64\n","Estimated Total Size (MB): 694.03\n","----------------------------------------------------------------\n"]}],"source":["summary(ConvAutoencoder_v2().to(device),(3,512,512))"]},{"cell_type":"markdown","metadata":{},"source":["# Training Function"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:43.699839Z","iopub.status.busy":"2023-10-06T06:04:43.699481Z","iopub.status.idle":"2023-10-06T06:04:43.725336Z","shell.execute_reply":"2023-10-06T06:04:43.724381Z","shell.execute_reply.started":"2023-10-06T06:04:43.699801Z"},"trusted":true},"outputs":[],"source":["def load_ckpt(checkpoint_fpath, model, optimizer):\n","    \n","    # load check point\n","    checkpoint = torch.load(checkpoint_fpath)\n","\n","    # initialize state_dict from checkpoint to model\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    # initialize optimizer from checkpoint to optimizer\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    # initialize valid_loss_min from checkpoint to valid_loss_min\n","    #valid_loss_min = checkpoint['valid_loss_min']\n","\n","    # return model, optimizer, epoch value, min validation loss \n","    return model, optimizer, checkpoint['epoch']\n","\n","def save_checkpoint(state, filename):\n","    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n","    print (\"=> Saving a new best\")\n","    torch.save(state, filename)  # save checkpoint\n","    \n","def train_model(model,  \n","                criterion, \n","                optimizer, \n","                #scheduler, \n","                num_epochs):\n","    since = time.time()\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss = np.inf\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","\n","            # Iterate over data.\n","            for idx,inputs in enumerate(Bar(dataloaders[phase])):\n","                inputs = inputs.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs, output_dec = model(inputs)\n","                    loss = criterion(output_dec, inputs)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","            #if phase == 'train':\n","            #    scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","\n","            print('{} Loss: {:.4f}'.format(\n","                phase, epoch_loss))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_loss < best_loss:\n","                best_loss = epoch_loss\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                save_checkpoint(state={   \n","                                    'epoch': epoch,\n","                                    'state_dict': model.state_dict(),\n","                                    'best_loss': best_loss,\n","                                    'optimizer_state_dict':optimizer.state_dict()\n","                                },filename='ckpt_epoch_{}.pt'.format(epoch))\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Loss: {:4f}'.format(best_loss))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, optimizer, epoch_loss"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:43.731730Z","iopub.status.busy":"2023-10-06T06:04:43.729083Z","iopub.status.idle":"2023-10-06T06:04:43.800954Z","shell.execute_reply":"2023-10-06T06:04:43.800088Z","shell.execute_reply.started":"2023-10-06T06:04:43.731691Z"},"trusted":true},"outputs":[],"source":["EPOCHS = 50\n","NUM_BATCHES = 32\n","RETRAIN = False\n","\n","train_set, validate_set = prepare_data(DF=df)\n","\n","dataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n","                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n","                }\n","\n","dataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n","\n","model = ConvAutoencoder_v2().to(device)\n","\n","criterion = nn.MSELoss()\n","# Observe that all parameters are being optimized\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n","learning_rate = 0.05\n","weight_decay = 0.0001\n","\n","# Create the SGD optimizer with weight decay\n","# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","# Decay LR by a factor of 0.1 every 7 epochs\n","#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:43.807345Z","iopub.status.busy":"2023-10-06T06:04:43.805158Z","iopub.status.idle":"2023-10-06T06:04:43.813882Z","shell.execute_reply":"2023-10-06T06:04:43.812714Z","shell.execute_reply.started":"2023-10-06T06:04:43.807298Z"},"trusted":true},"outputs":[],"source":["# If re-training is required:\n","# Load the old model\n","if RETRAIN == True:\n","    # load the saved checkpoint\n","    model, optimizer, start_epoch = load_ckpt('../input/cbirpretrained/conv_autoencoder.pt', model, optimizer)\n","    print('Checkpoint Loaded')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-06T06:04:43.820863Z","iopub.status.busy":"2023-10-06T06:04:43.818240Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0/50\n","----------\n","4027/4027: [===============================>] - ETA 1.4sss\n","train Loss: 0.1348\n","711/711: [==============================>.] - ETA 0.8ss\n","val Loss: 0.0702\n","=> Saving a new best\n","\n","Epoch 1/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0626\n","711/711: [==============================>.] - ETA 0.7ss\n","val Loss: 0.0564\n","=> Saving a new best\n","\n","Epoch 2/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0531\n","711/711: [==============================>.] - ETA 0.6ss\n","val Loss: 0.0475\n","=> Saving a new best\n","\n","Epoch 3/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0435\n","711/711: [==============================>.] - ETA 0.8ss\n","val Loss: 0.0410\n","=> Saving a new best\n","\n","Epoch 4/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0397\n","711/711: [==============================>.] - ETA 0.6ss\n","val Loss: 0.0381\n","=> Saving a new best\n","\n","Epoch 5/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0367\n","711/711: [==============================>.] - ETA 0.7ss\n","val Loss: 0.0352\n","=> Saving a new best\n","\n","Epoch 6/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0345\n","711/711: [==============================>.] - ETA 0.7ss\n","val Loss: 0.0337\n","=> Saving a new best\n","\n","Epoch 7/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0327\n","711/711: [==============================>.] - ETA 0.7ss\n","val Loss: 0.0319\n","=> Saving a new best\n","\n","Epoch 8/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0312\n","711/711: [==============================>.] - ETA 0.6ss\n","val Loss: 0.0315\n","=> Saving a new best\n","\n","Epoch 9/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0299\n","711/711: [==============================>.] - ETA 0.7ss\n","val Loss: 0.0293\n","=> Saving a new best\n","\n","Epoch 10/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0290\n","711/711: [==============================>.] - ETA 0.7ss\n","val Loss: 0.0282\n","=> Saving a new best\n","\n","Epoch 11/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0282\n","711/711: [==============================>.] - ETA 0.7ss\n","val Loss: 0.0279\n","=> Saving a new best\n","\n","Epoch 12/50\n","----------\n","4027/4027: [===============================>] - ETA 1.0sss\n","train Loss: 0.0271\n","711/711: [==============================>.] - ETA 0.7ss\n","val Loss: 0.0271\n","=> Saving a new best\n","\n","Epoch 13/50\n","----------\n","3040/4027: [=======================>........] - ETA 33.3ss"]}],"source":["model, optimizer, loss = train_model(model=model, \n","                    criterion=criterion, \n","                    optimizer=optimizer, \n","                    #scheduler=exp_lr_scheduler,\n","                    num_epochs=EPOCHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save the Trained Model\n","torch.save({\n","            'epoch': EPOCHS,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, 'conv_autoencoderv2_5ep.pt')"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Indexing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["transformations = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load Model in Evaluation phase\n","model = ConvAutoencoder_v2().to(device)\n","model.load_state_dict(torch.load('/kaggle/working/conv_autoencoderv2_5ep.pt', map_location=device)['model_state_dict'], strict=False)\n","\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def top_n_local_features( local_features, n = 200 ):\n","    # Step 1: Compute L2 norms for each feature vector\n","    norms = np.linalg.norm(local_features, axis=(1, 2))  # Compute norms along the feature dimensions\n","\n","    # Step 2: Get the indices of the top 'n' features based on L2 norm\n","#     n = 150  # Replace with the number of top features you want to select\n","    top_indices = np.argsort(norms)[-n:]\n","\n","    # Step 3: Select the top 'n' local features\n","    top_n_local_features = local_features[top_indices]\n","    \n","#     print(top_n_local_features.shape)\n","    return top_n_local_features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","# Define a sample input as a numpy array\n","sample_input = np.random.randn( 256, 16, 16)\n","\n","# Call the top_n_local_features function\n","output = top_n_local_features(sample_input)\n","\n","# Print the shape of the output\n","print(\"Output shape:\", output.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_latent_features(images, transformations):\n","    n = 200\n","#     latent_features = np.zeros((4738,256,16,16))\n","    latent_features = np.zeros((4738,256,16,16))\n","    top_latent_features = np.zeros((4738,n,16,16))\n","    #latent_features = np.zeros((4738,8,42,42))\n","    gem = GeM()\n","    \n","    for i,image in enumerate(tqdm(images)):\n","        \n","        tensor = transformations(Image.open(image)).to(device)\n","        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n","#         print( type(latent_features[i]), latent_features[i].shape )\n","        # Top N = 150 local feature selection\n","        top_latent_features[i] = top_n_local_features( latent_features[i], n = n )\n","        top_latent_features[i] = gem(torch.tensor( top_latent_features[i] ) )\n","        \n","    del tensor\n","    gc.collect()\n","    return top_latent_features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images = df.image.values\n","latent_features = get_latent_features(images, transformations)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["indexes = list(range(0, 4738))\n","feature_dict = dict(zip(indexes,latent_features))\n","index_dict = {'indexes':indexes,'features':latent_features}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# write the data dictionary to disk\n","#with open('features.pkl', \"wb\") as f:\n","#    f.write(pickle.dumps(index_dict))"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Image Retrieval "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def euclidean(a, b):\n","    # compute and return the euclidean distance between two vectors\n","#     print(a.shape, b.shape)\n","    return np.linalg.norm(a - b)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from numpy.linalg import norm\n","\n","def cosine_similarity(a,b):\n","    # Reshape the arrays into 1D vectors\n","    a_flat = a.reshape(-1)\n","    b_flat = b.reshape(-1)\n","\n","    # Compute the cosine similarity\n","    cos_sim = np.dot(a_flat, b_flat) / (norm(a_flat) * norm(b_flat))\n","    return cos_sim\n","# def cosine_distance(a,b):\n","#     return scipy.spatial.distance.cosine(a, b)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def perform_search(queryFeatures, index, maxResults=64):\n","\n","    results = []\n","    similarity = []\n","\n","    for i in range(0, len(index[\"features\"])):\n","        # compute the euclidean distance between our query features\n","        # and the features for the current image in our index, then\n","        # update our results list with a 2-tuple consisting of the\n","        # computed distance and the index of the image\n","        d = euclidean(queryFeatures, index[\"features\"][i])\n","#         d = cosine_distance( queryFeatures, index[\"features\"][i] )\n","        sim = cosine_similarity( queryFeatures, index[\"features\"][i] )\n","        results.append((d, i))\n","        similarity.append((sim, i))\n","    \n","#     # sort the results and grab the top ones\n","# #     results = sorted(results)[:maxResults]\n","#      # Sort similarity in descending order based on the similarity score\n","#     similarity = sorted(similarity, key=lambda x: x[0], reverse=True)\n","#     # Grab the top ones based on maxResults\n","#     similarity = similarity[:maxResults]\n","# #     print(results)\n","# #     print(similarity)\n","#     # return the list of results\n","# #     return results\n","#     return similarity\n","     # Sort similarity in descending order based on the similarity score\n","    similarity = sorted(similarity, key=lambda x: x[0], reverse=True)\n","\n","    # Apply softmax to the similarity scores\n","    similarity_scores = np.array([item[0] for item in similarity])\n","    softmax_similarity = np.exp(similarity_scores - np.max(similarity_scores)) / np.sum(np.exp(similarity_scores - np.max(similarity_scores)))\n","\n","    # Combine the softmax scores with their corresponding indices\n","    softmax_similarity = [(softmax_similarity[i], similarity[i][1]) for i in range(len(similarity))]\n","\n","    # Grab the top ones based on maxResults\n","    softmax_similarity = softmax_similarity[:maxResults]\n","    \n","    return softmax_similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_montages(image_list, image_shape, montage_shape):\n","\n","    if len(image_shape) != 2:\n","        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n","    if len(montage_shape) != 2:\n","        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n","    image_montages = []\n","    # start with black canvas to draw images onto\n","    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n","                          dtype=np.uint8)\n","    cursor_pos = [0, 0]\n","    start_new_img = False\n","    for img in image_list:\n","        if type(img).__module__ != np.__name__:\n","            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n","        start_new_img = False\n","        img = cv2.resize(img, image_shape)\n","        # draw image to black canvas\n","        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n","        cursor_pos[0] += image_shape[0]  # increment cursor x position\n","        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n","            cursor_pos[1] += image_shape[1]  # increment cursor y position\n","            cursor_pos[0] = 0\n","            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n","                cursor_pos = [0, 0]\n","                image_montages.append(montage_image)\n","                # reset black canvas\n","                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n","                                      dtype=np.uint8)\n","                start_new_img = True\n","    if start_new_img is False:\n","        image_montages.append(montage_image)  # add unfinished montage\n","    return image_montages"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# take the features for the current image, find all similar\n","# images in our dataset, and then initialize our list of result\n","# images\n","fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n","queryIdx = 3166# Input Index for which images \n","MAX_RESULTS = 10\n","\n","\n","queryFeatures = latent_features[queryIdx]\n","print( queryFeatures.shape )\n","results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n","imgs = []\n","\n","# loop over the results\n","for (d, j) in results:\n","    img = np.array(Image.open(images[j]))\n","    print(j)\n","    imgs.append(img)\n","\n","# display the query image\n","ax[0].imshow(np.array(Image.open(images[queryIdx])))\n","\n","# build a montage from the results and display it\n","montage = build_montages(imgs, (512, 512), (5, 2))[0]\n","ax[1].imshow(montage)"]},{"cell_type":"markdown","metadata":{},"source":["# Clustering of Images"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from sklearn.cluster import KMeans, MiniBatchKMeans\n","# from scipy.spatial.distance import cdist\n","# from sklearn.metrics import silhouette_samples, silhouette_score\n","\n","# import matplotlib.cm as cm\n","# %matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-06T06:03:05.598336Z","iopub.status.idle":"2023-10-06T06:03:05.599044Z"},"trusted":true},"outputs":[],"source":["# def get_latent_features1D(images, transformations):\n","    \n","#     latent_features1d = []\n","    \n","#     for i,image in enumerate(tqdm(images)):\n","#         tensor = transformations(Image.open(image)).to(device)\n","#         latent_features1d.append(model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy().flatten())\n","        \n","#     del tensor\n","#     gc.collect()\n","#     return latent_features1d"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-06T06:03:05.600116Z","iopub.status.idle":"2023-10-06T06:03:05.600843Z"},"trusted":true},"outputs":[],"source":["# images = df.image.values\n","# latent_features1d = get_latent_features1D(images, transformations)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-06T06:03:05.601927Z","iopub.status.idle":"2023-10-06T06:03:05.602660Z"},"trusted":true},"outputs":[],"source":["# latent_features1d = np.array(latent_features1d)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-06T06:03:05.603754Z","iopub.status.idle":"2023-10-06T06:03:05.604498Z"},"trusted":true},"outputs":[],"source":["# distortions = [] \n","# inertias = [] \n","# mapping1 = {} \n","# mapping2 = {} \n","# K = range(10,11) \n","  \n","# for k in tqdm(K): \n","#     #Building and fitting the model \n","#     kmeanModel = KMeans(n_clusters=k).fit(latent_features1d)      \n","      \n","#     distortions.append(sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_, \n","#                       'euclidean'),axis=1)) / latent_features1d.shape[0]) \n","#     inertias.append(kmeanModel.inertia_) \n","  \n","#     mapping1[k] = sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_, \n","#                  'euclidean'),axis=1)) / latent_features1d.shape[0] \n","#     mapping2[k] = kmeanModel.inertia_ "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-06T06:03:05.605565Z","iopub.status.idle":"2023-10-06T06:03:05.606281Z"},"trusted":true},"outputs":[],"source":["# plt.plot(K, distortions, 'bx-') \n","# plt.xlabel('Values of K') \n","# plt.ylabel('Distortion') \n","# plt.title('The Elbow Method using Distortion') \n","# plt.show() "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-06T06:03:05.607438Z","iopub.status.idle":"2023-10-06T06:03:05.608135Z"},"trusted":true},"outputs":[],"source":["# X = np.array(latent_features1d)\n","# K = range(10, 11) \n","\n","# for n_clusters in tqdm(K):\n","#     # Create a subplot with 1 row and 2 columns\n","#     fig, (ax1, ax2) = plt.subplots(1, 2)\n","#     fig.set_size_inches(18, 7)\n","\n","#     # The 1st subplot is the silhouette plot\n","#     # The silhouette coefficient can range from -1, 1 but in this example all\n","#     # lie within [-0.1, 1]\n","#     ax1.set_xlim([-0.1, 1])\n","#     # The (n_clusters+1)*10 is for inserting blank space between silhouette\n","#     # plots of individual clusters, to demarcate them clearly.\n","#     ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n","\n","#     # Initialize the clusterer with n_clusters value and a random generator\n","#     # seed of 10 for reproducibility.\n","#     clusterer = KMeans(n_clusters=n_clusters, random_state=RANDOMSTATE)\n","#     cluster_labels = clusterer.fit_predict(X)\n","\n","#     # The silhouette_score gives the average value for all the samples.\n","#     # This gives a perspective into the density and separation of the formed\n","#     # clusters\n","#     silhouette_avg = silhouette_score(X, cluster_labels)\n","#     print(\"For n_clusters =\", n_clusters,\n","#           \"The average silhouette_score is :\", silhouette_avg)\n","\n","#     # Compute the silhouette scores for each sample\n","#     sample_silhouette_values = silhouette_samples(X, cluster_labels)\n","\n","#     y_lower = 10\n","#     for i in range(n_clusters):\n","#         # Aggregate the silhouette scores for samples belonging to\n","#         # cluster i, and sort them\n","#         ith_cluster_silhouette_values = \\\n","#             sample_silhouette_values[cluster_labels == i]\n","\n","#         ith_cluster_silhouette_values.sort()\n","\n","#         size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","#         y_upper = y_lower + size_cluster_i\n","\n","#         color = cm.nipy_spectral(float(i) / n_clusters)\n","#         ax1.fill_betweenx(np.arange(y_lower, y_upper),\n","#                           0, ith_cluster_silhouette_values,\n","#                           facecolor=color, edgecolor=color, alpha=0.7)\n","\n","#         # Label the silhouette plots with their cluster numbers at the middle\n","#         ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","#         # Compute the new y_lower for next plot\n","#         y_lower = y_upper + 10  # 10 for the 0 samples\n","\n","#     ax1.set_title(\"The silhouette plot for the various clusters.\")\n","#     ax1.set_xlabel(\"The silhouette coefficient values\")\n","#     ax1.set_ylabel(\"Cluster label\")\n","\n","#     # The vertical line for average silhouette score of all the values\n","#     ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","\n","#     ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","#     ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","#     # 2nd Plot showing the actual clusters formed\n","#     colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n","#     ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n","#                 c=colors, edgecolor='k')\n","\n","#     # Labeling the clusters\n","#     centers = clusterer.cluster_centers_\n","#     # Draw white circles at cluster centers\n","#     ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n","#                 c=\"white\", alpha=1, s=200, edgecolor='k')\n","\n","#     for i, c in enumerate(centers):\n","#         ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n","#                     s=50, edgecolor='k')\n","\n","#     ax2.set_title(\"The visualization of the clustered data.\")\n","#     ax2.set_xlabel(\"Feature space for the 1st feature\")\n","#     ax2.set_ylabel(\"Feature space for the 2nd feature\")\n","\n","#     plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n","#                   \"with n_clusters = %d\" % n_clusters),\n","#                  fontsize=14, fontweight='bold')\n","\n","# plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
